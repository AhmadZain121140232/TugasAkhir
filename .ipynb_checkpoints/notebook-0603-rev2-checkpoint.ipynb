{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library\n",
    "\n",
    "In this part of the code, we are importing various libraries and modules that we will use to build, train, and evaluate our machine learning models. Think of these libraries as tools in a toolbox that help us perform specific tasks more easily. Here’s a brief explanation of each import statement:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "- **pandas (`pd`)**: This library is used for data manipulation and analysis. It allows us to easily read, process, and analyze data stored in various formats, such as CSV files.\n",
    "- **numpy (`np`)**: This library is used for numerical computing. It provides support for arrays (lists of numbers) and mathematical functions to operate on these arrays.\n",
    "\n",
    "```python\n",
    "from sklearn... import ...\n",
    "```\n",
    "\n",
    "- **sklearn**: This library is used for machine learning tasks such as classification, regression, clustering, and more. It provides a wide range of tools and algorithms to build and evaluate machine learning models.\n",
    "\n",
    "```python\n",
    "from xgboost import XGBRegressor\n",
    "```\n",
    "\n",
    "- **XGBRegressor**: This is an optimized version of gradient boosting that includes additional features and optimizations. It is known for its high performance and efficiency, especially on larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Dataset\n",
    "\n",
    "In this part of the code, we are reading the dataset that contains the training and testing data for our machine learning model. The dataset is stored in a CSV (Comma-Separated Values) file, which is a common format for storing tabular data. We use the `pd.read_csv()` function from the `pandas` library to read the dataset into a DataFrame, which is a two-dimensional data structure that resembles a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Template_for_python S04.csv\")\n",
    "data = data.dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Features and Target\n",
    "\n",
    "In this part of the code, we are defining the features (input variables) and the target (output variable) that will be used to train our machine learning model. The features are the columns in the dataset that will be used to make predictions, while the target is the column that contains the values we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['clim_zone', 'build_orient', 'build_type',\n",
    "            'tot_floor_area', 'fac_glaz_uval', 'fac_glaz_shgc', 'wall_type', 'wall_uval',\n",
    "            'floor_finish', 'floor_uval', 'roof_type', 'roop_uval', 'infilt_type', 'infilt_ach',\n",
    "            'epd', 'lpd', 'hvac_type', 'pv_pe', 'pv_po', 'pv_pt', 'pv_pa',\n",
    "            'res_hedc', 'res_cedc']\n",
    "target = 'res_eui'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Non-Numerical Features\n",
    "\n",
    "Not all features in the dataset are numerical. Some features may be categorical or text-based, which need to be converted into numerical values before they can be used in a machine learning model. In this part of the code, we are preprocessing non-numerical features by converting them into numerical values using techniques such as one-hot encoding.\n",
    "\n",
    "## About One-Hot Encoding\n",
    "\n",
    "One-hot encoding is a technique used to convert categorical variables into a numerical format that can be used in machine learning models. It works by creating a binary column for each category in the categorical variable, where each column indicates the presence or absence of that category. This allows the model to learn the relationship between the categories and the target variable.\n",
    "\n",
    "### One-Hot Encoding Explained\n",
    "\n",
    "One-hot encoding is a technique used to convert categorical variables into a numerical format that machine learning models can understand. When you have a column with categorical data, such as colors (red, green, blue), one-hot encoding creates new binary (0 or 1) columns for each category. This way, the data can be easily used by machine learning algorithms.\n",
    "\n",
    "#### Example: One-Hot Encoding\n",
    "\n",
    "Let's say we have a dataset of building types with a column called `build_type`. This column has three categories: \"Residential\", \"Commercial\", and \"Industrial\".\n",
    "\n",
    "Here’s what the original data might look like:\n",
    "\n",
    "| build_type  |\n",
    "|-------------|\n",
    "| Residential |\n",
    "| Commercial  |\n",
    "| Industrial  |\n",
    "| Residential |\n",
    "| Commercial  |\n",
    "\n",
    "Using one-hot encoding, we will convert this single column into three new columns, one for each category. Each column will contain binary values indicating whether the original `build_type` was that category.\n",
    "\n",
    "| build_type_Residential | build_type_Commercial | build_type_Industrial |\n",
    "|------------------------|-----------------------|-----------------------|\n",
    "| 1                      | 0                     | 0                     |\n",
    "| 0                      | 1                     | 0                     |\n",
    "| 0                      | 0                     | 1                     |\n",
    "| 1                      | 0                     | 0                     |\n",
    "| 0                      | 1                     | 0                     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['clim_zone', 'build_type', 'wall_type', 'floor_finish', 'roof_type', 'infilt_type', 'hvac_type']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data Into Training and Testing\n",
    "\n",
    "Why we split the data into training and testing sets?\n",
    "\n",
    "When building a machine learning model, it is important to evaluate its performance on data that it has not seen before. This helps us understand how well the model generalizes to new, unseen data. To achieve this, we split the dataset into two parts: a training set and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[features], data[target], test_size=0.2, random_state=2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model\n",
    "\n",
    "In this experiment, I tried to simultaneously use five models: LinearRegression, XGBoost, RandomForest, GradientBoosting, and Support Vector Machine. The model that performs best will be used to make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "model_xgb = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=2024))\n",
    "])\n",
    "\n",
    "model_rfr = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=2024))\n",
    "])\n",
    "\n",
    "model_gbr = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', GradientBoostingRegressor(n_estimators=100, random_state=2024))\n",
    "])\n",
    "\n",
    "model_svr = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', SVR(kernel='rbf', C=1.0, epsilon=0.1))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "In this part of the code, we are training the machine learning model using the training data. The model learns the relationship between the input features and the target variable by adjusting its internal parameters based on the training data. The goal is to minimize the error between the predicted values and the actual values in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr.fit(X_train, y_train)\n",
    "y_pred_lr = model_lr.predict(X_test)\n",
    "\n",
    "model_xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = model_xgb.predict(X_test)\n",
    "\n",
    "model_rfr.fit(X_train, y_train)\n",
    "y_pred_rfr = model_rfr.predict(X_test)\n",
    "\n",
    "model_gbr.fit(X_train, y_train)\n",
    "y_pred_gbr = model_gbr.predict(X_test)\n",
    "\n",
    "model_svr.fit(X_train, y_train)\n",
    "y_pred_svr = model_svr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model\n",
    "\n",
    "We use three metrics to evaluate the performance of the model:\n",
    "* R2 Score: It is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. The R2 score ranges from 0 to 1, where 1 indicates a perfect fit and 0 indicates no relationship between the independent and dependent variables.\n",
    "* RMSE (Root Mean Squared Error): It is a measure of the differences between values predicted by a model and the values observed. It is the square root of the average of the squared differences between the predicted and actual values.\n",
    "* MAE (Mean Absolute Error): It is a measure of errors between paired observations expressing the same phenomenon. It is the average of the absolute differences between the predicted and actual values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared LR: -1.2379412783092114\n",
      "RMSE LR: 42.23415453242447\n",
      "MAE LR: 8.107548112454621\n",
      "\n",
      "R-squared XGB: 0.975441954274327\n",
      "RMSE XGB: 4.424216418067638\n",
      "MAE XGB: 3.0005763924640156\n",
      "\n",
      "R-squared RFR: 0.9543418718411509\n",
      "RMSE RFR: 6.032516419192416\n",
      "MAE RFR: 3.7163043478260867\n",
      "\n",
      "R-squared GBR: 0.9905612788203739\n",
      "RMSE GBR: 2.7428119934884907\n",
      "MAE GBR: 1.8863877913816305\n",
      "\n",
      "R-squared SVR: 0.07855218632395455\n",
      "RMSE SVR: 27.10034584366519\n",
      "MAE SVR: 16.35101037702196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "mae_lr = np.mean(np.abs(y_test - y_pred_lr))\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "mae_xgb = np.mean(np.abs(y_test - y_pred_xgb))\n",
    "r2_rfr = r2_score(y_test, y_pred_rfr)\n",
    "rmse_rfr = np.sqrt(mean_squared_error(y_test, y_pred_rfr))\n",
    "mae_rfr = np.mean(np.abs(y_test - y_pred_rfr))\n",
    "r2_gbr = r2_score(y_test, y_pred_gbr)\n",
    "rmse_gbr = np.sqrt(mean_squared_error(y_test, y_pred_gbr))\n",
    "mae_gbr = np.mean(np.abs(y_test - y_pred_gbr))\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "rmse_svr = np.sqrt(mean_squared_error(y_test, y_pred_svr))\n",
    "mae_svr = np.mean(np.abs(y_test - y_pred_svr))\n",
    "\n",
    "print(f'R-squared LR: {r2_lr}')\n",
    "print(f'RMSE LR: {rmse_lr}')\n",
    "print(f'MAE LR: {mae_lr}\\n')\n",
    "print(f'R-squared XGB: {r2_xgb}')\n",
    "print(f'RMSE XGB: {rmse_xgb}')\n",
    "print(f'MAE XGB: {mae_xgb}\\n')\n",
    "print(f'R-squared RFR: {r2_rfr}')\n",
    "print(f'RMSE RFR: {rmse_rfr}')\n",
    "print(f'MAE RFR: {mae_rfr}\\n')\n",
    "print(f'R-squared GBR: {r2_gbr}')\n",
    "print(f'RMSE GBR: {rmse_gbr}')\n",
    "print(f'MAE GBR: {mae_gbr}\\n')\n",
    "print(f'R-squared SVR: {r2_svr}')\n",
    "print(f'RMSE SVR: {rmse_svr}')\n",
    "print(f'MAE SVR: {mae_svr}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results of the evaluation metrics, we can determine how well the model is performing and make adjustments as needed to improve its performance. From the five models, LinearRegression performed the best, with the highest R2 score and lowest RMSE and MAE values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Feature Importance\n",
    "(For Linear Regression Model)\n",
    "\n",
    "We want to check, from all of the features, which features are the most important in predicting the target variable. This can help us understand which features have the most impact on the target variable and how they contribute to the model's predictions.\n",
    "\n",
    "If the coefficients of the features are positive, it means that the feature has a positive impact on the target variable. If the coefficients are negative, it means that the feature has a negative impact (inverse) on the target variable.\n",
    "\n",
    "We will sort the features based on their coefficients to identify the most important features in the model.\n",
    "\n",
    "For the categorical features (with prefix `cat`), there will be a number of categories in the end of the feature. E.g.: `cat_hvac_type_0.0, means that the feature is `hvac_type` with category `0.0`, in this case is `Fan Coil Units and Central Plant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features affecting res_eui (sorted by importance):\n",
      "cat__clim_zone_7: 7291149589.899384\n",
      "cat__clim_zone_6A: 7291149500.67927\n",
      "cat__clim_zone_5A: 7291149483.058145\n",
      "cat__clim_zone_1A: 7291149473.050152\n",
      "cat__clim_zone_0A: 7291149471.1863575\n",
      "cat__clim_zone_3A: 7291149432.756974\n",
      "cat__clim_zone_2A: 7291149429.235195\n",
      "cat__clim_zone_4A: 7291149424.924022\n",
      "remainder__fac_glaz_shgc: -20.779385890152607\n",
      "remainder__infilt_ach: 10.180511458910336\n",
      "cat__hvac_type_0.0: 5.737593489896991\n",
      "cat__hvac_type_1.0: -5.737487130118826\n",
      "remainder__floor_uval: 3.816983844884436\n",
      "remainder__pv_pe: -3.7699506650868124\n",
      "cat__floor_finish_0.0: -2.503824107191431\n",
      "remainder__roop_uval: 2.337979738717343\n",
      "remainder__fac_glaz_uval: 1.9584975011792722\n",
      "remainder__lpd: 1.7526194870627188\n",
      "remainder__epd: 1.6132309276214314\n",
      "cat__floor_finish_1.0: 1.405026520687813\n",
      "cat__floor_finish_2.0: 1.0987776807509166\n",
      "cat__wall_type_0.0: -0.91877364032667\n",
      "cat__wall_type_2.0: 0.8213032534082229\n",
      "cat__roof_type_0.0: -0.570305228711857\n",
      "cat__roof_type_1.0: 0.5702323254399365\n",
      "remainder__wall_uval: 0.3554557051434239\n",
      "remainder__pv_pt: -0.1976448435168179\n",
      "cat__wall_type_1.0: 0.09738499751237327\n",
      "remainder__pv_po: -0.08523751105898658\n",
      "remainder__res_hedc: 0.050442952823524595\n",
      "remainder__res_cedc: 0.03139035129515444\n",
      "remainder__pv_pa: -0.01655455667672263\n",
      "remainder__tot_floor_area: -0.001511176573687721\n",
      "remainder__build_orient: 0.0013882465626060666\n",
      "cat__build_type_0.0: -0.0004380941390991211\n",
      "cat__infilt_type_0.0: 2.5510787963867188e-05\n"
     ]
    }
   ],
   "source": [
    "coefficients = model_lr.named_steps['regressor'].coef_\n",
    "feature_names = model_lr.named_steps['preprocessor'].get_feature_names_out()\n",
    "feature_importance = dict(zip(feature_names, coefficients))\n",
    "\n",
    "sorted_feature_importance = dict(sorted(feature_importance.items(), key=lambda item: abs(item[1]), reverse=True))\n",
    "\n",
    "print(\"\\nFeatures affecting res_eui (sorted by importance):\")\n",
    "for feature, coefficient in sorted_feature_importance.items():\n",
    "    print(f'{feature}: {coefficient}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(For Gradient Boosting Regression Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features affecting res_eui (Gradient Boosting Regressor). Sorted by importance\n",
      "remainder__pv_pa: 0.5695905170936769\n",
      "remainder__res_hedc: 0.18472917478490003\n",
      "cat__clim_zone_7: 0.1194466428518721\n",
      "remainder__res_cedc: 0.023711789699299095\n",
      "remainder__tot_floor_area: 0.022852491829502566\n",
      "cat__clim_zone_6A: 0.01858816820490199\n",
      "remainder__pv_pe: 0.015908643827879165\n",
      "cat__clim_zone_4A: 0.012727590462598127\n",
      "remainder__pv_po: 0.00944859235705199\n",
      "remainder__epd: 0.004222788575979729\n",
      "cat__clim_zone_5A: 0.0038962808340912594\n",
      "cat__floor_finish_0.0: 0.003835188628237253\n",
      "remainder__floor_uval: 0.0024581858679852808\n",
      "remainder__lpd: 0.002160910369521605\n",
      "remainder__fac_glaz_uval: 0.0014637582245279849\n",
      "cat__hvac_type_0.0: 0.0010748801839394852\n",
      "remainder__roop_uval: 0.000852680893810155\n",
      "remainder__infilt_ach: 0.0008018017707504432\n",
      "remainder__pv_pt: 0.0007026500362272205\n",
      "cat__hvac_type_1.0: 0.0006427481135194974\n",
      "cat__roof_type_0.0: 0.00037732459818478703\n",
      "cat__roof_type_1.0: 0.00018534074496415008\n",
      "cat__clim_zone_3A: 0.00011879367303721428\n",
      "cat__floor_finish_1.0: 9.732441808031615e-05\n",
      "cat__floor_finish_2.0: 9.667959281849053e-05\n",
      "cat__clim_zone_1A: 6.865758776305248e-06\n",
      "remainder__wall_uval: 1.418390013550031e-06\n",
      "cat__clim_zone_0A: 7.68213853286606e-07\n",
      "cat__clim_zone_2A: 0.0\n",
      "cat__build_type_0.0: 0.0\n",
      "cat__wall_type_0.0: 0.0\n",
      "cat__wall_type_1.0: 0.0\n",
      "cat__wall_type_2.0: 0.0\n",
      "cat__infilt_type_0.0: 0.0\n",
      "remainder__build_orient: 0.0\n",
      "remainder__fac_glaz_shgc: 0.0\n"
     ]
    }
   ],
   "source": [
    "feature_importances_gbr = model_gbr.named_steps['regressor'].feature_importances_\n",
    "feature_names_gbr = model_gbr.named_steps['preprocessor'].get_feature_names_out()\n",
    "feature_importance_gbr = dict(zip(feature_names_gbr, feature_importances_gbr))\n",
    "\n",
    "sorted_feature_importance_gbr = dict(sorted(feature_importance_gbr.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "print(\"\\nFeatures affecting res_eui (Gradient Boosting Regressor). Sorted by importance\")\n",
    "for feature, importance in sorted_feature_importance_gbr.items():\n",
    "    print(f'{feature}: {importance}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
