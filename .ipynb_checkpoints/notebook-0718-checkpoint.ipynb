{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library\n",
    "\n",
    "In this part of the code, we are importing various libraries and modules that we will use to build, train, and evaluate our machine learning models. Think of these libraries as tools in a toolbox that help us perform specific tasks more easily. Here’s a brief explanation of each import statement:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "- **pandas (`pd`)**: This library is used for data manipulation and analysis. It allows us to easily read, process, and analyze data stored in various formats, such as CSV files.\n",
    "- **numpy (`np`)**: This library is used for numerical computing. It provides support for arrays (lists of numbers) and mathematical functions to operate on these arrays.\n",
    "\n",
    "```python\n",
    "from sklearn... import ...\n",
    "```\n",
    "\n",
    "- **sklearn**: This library is used for machine learning tasks such as classification, regression, clustering, and more. It provides a wide range of tools and algorithms to build and evaluate machine learning models.\n",
    "\n",
    "```python\n",
    "from xgboost import XGBRegressor\n",
    "```\n",
    "\n",
    "- **XGBRegressor**: This is an optimized version of gradient boosting that includes additional features and optimizations. It is known for its high performance and efficiency, especially on larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Dataset\n",
    "\n",
    "In this part of the code, we are reading the dataset that contains the training and testing data for our machine learning model. The dataset is stored in a CSV (Comma-Separated Values) file, which is a common format for storing tabular data. We use the `pd.read_csv()` function from the `pandas` library to read the dataset into a DataFrame, which is a two-dimensional data structure that resembles a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Template_for_python S04.csv\")\n",
    "data = data.dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Features and Target\n",
    "\n",
    "In this part of the code, we are defining the features (input variables) and the target (output variable) that will be used to train our machine learning model. The features are the columns in the dataset that will be used to make predictions, while the target is the column that contains the values we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['clim_zone', 'build_orient',\n",
    "            'tot_floor_area', 'fac_glaz_uval', 'fac_glaz_shgc', 'wall_type', 'wall_uval',\n",
    "            'floor_finish', 'floor_uval', 'roof_type', 'roof_uval', 'infilt_ach',\n",
    "            'epd', 'lpd', 'hvac_type', 'pv_pe', 'pv_po', 'pv_pt', 'pv_pa',\n",
    "            'res_hedc', 'res_cedc']\n",
    "target = 'res_eui'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features: 21\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Features: {len(features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Non-Numerical Features\n",
    "\n",
    "Not all features in the dataset are numerical. Some features may be categorical or text-based, which need to be converted into numerical values before they can be used in a machine learning model. In this part of the code, we are preprocessing non-numerical features by converting them into numerical values using techniques such as one-hot encoding.\n",
    "\n",
    "## About One-Hot Encoding\n",
    "\n",
    "One-hot encoding is a technique used to convert categorical variables into a numerical format that can be used in machine learning models. It works by creating a binary column for each category in the categorical variable, where each column indicates the presence or absence of that category. This allows the model to learn the relationship between the categories and the target variable.\n",
    "\n",
    "### One-Hot Encoding Explained\n",
    "\n",
    "One-hot encoding is a technique used to convert categorical variables into a numerical format that machine learning models can understand. When you have a column with categorical data, such as colors (red, green, blue), one-hot encoding creates new binary (0 or 1) columns for each category. This way, the data can be easily used by machine learning algorithms.\n",
    "\n",
    "#### Example: One-Hot Encoding\n",
    "\n",
    "Let's say we have a dataset of building types with a column called `build_type`. This column has three categories: \"Residential\", \"Commercial\", and \"Industrial\".\n",
    "\n",
    "Here’s what the original data might look like:\n",
    "\n",
    "| build_type  |\n",
    "|-------------|\n",
    "| Residential |\n",
    "| Commercial  |\n",
    "| Industrial  |\n",
    "| Residential |\n",
    "| Commercial  |\n",
    "\n",
    "Using one-hot encoding, we will convert this single column into three new columns, one for each category. Each column will contain binary values indicating whether the original `build_type` was that category.\n",
    "\n",
    "| build_type_Residential | build_type_Commercial | build_type_Industrial |\n",
    "|------------------------|-----------------------|-----------------------|\n",
    "| 1                      | 0                     | 0                     |\n",
    "| 0                      | 1                     | 0                     |\n",
    "| 0                      | 0                     | 1                     |\n",
    "| 1                      | 0                     | 0                     |\n",
    "| 0                      | 1                     | 0                     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['clim_zone', 'wall_type', 'floor_finish', 'roof_type', 'hvac_type']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data Into 5 Folds\n",
    "\n",
    "```python\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=2024)\n",
    "```\n",
    "\n",
    "This line initializes a K-Fold cross-validator from scikit-learn's model selection module. Let's break down the parameters:\n",
    "\n",
    "- `n_splits=5`: This sets up 5-fold cross-validation. The data will be divided into 5 equal parts or \"folds\".\n",
    "- `shuffle=True`: This parameter tells the cross-validator to shuffle the data before splitting it into folds. This helps to ensure that the order of the data doesn't affect the results.\n",
    "- `random_state=2024`: This sets a specific random seed for reproducibility. Using the same random state will ensure that the data is shuffled in the same way every time the code is run.\n",
    "\n",
    "In 5-fold cross-validation:\n",
    "1. The data is divided into 5 equal subsets or folds.\n",
    "2. The model is trained on 4 folds and tested on the remaining fold.\n",
    "3. This process is repeated 5 times, with each fold serving as the test set exactly once.\n",
    "4. The performance metrics are then averaged across all 5 iterations.\n",
    "\n",
    "This method helps to get a more robust estimate of the model's performance by using all the data for both training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model\n",
    "\n",
    "In this experiment, I tried to simultaneously use five models: LinearRegression, XGBoost, RandomForest, GradientBoosting, and Support Vector Machine. The model that performs best will be used to make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'LR': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', LinearRegression())]),\n",
    "    'XGB': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=2024))]),\n",
    "    'RFR': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', RandomForestRegressor(n_estimators=100, random_state=2024))]),\n",
    "    'GBR': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', GradientBoostingRegressor(n_estimators=100, random_state=2024))]),\n",
    "    'SVR': Pipeline(steps=[('preprocessor', preprocessor), ('regressor', SVR(kernel='rbf', C=1.0, epsilon=0.1))])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {model: {'r2': [], 'rmse': [], 'mae': []} for model in models}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "In this part of the code, we are training the machine learning model using the training data. The model learns the relationship between the input features and the target variable by adjusting its internal parameters based on the training data. The goal is to minimize the error between the predicted values and the actual values in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in kf.split(data[features]):\n",
    "    X_train, X_test = data[features].iloc[train_index], data[features].iloc[test_index]\n",
    "    y_train, y_test = data[target].iloc[train_index], data[target].iloc[test_index]\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        results[name]['r2'].append(r2_score(y_test, y_pred))\n",
    "        results[name]['rmse'].append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "        results[name]['mae'].append(np.mean(np.abs(y_test - y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model\n",
    "\n",
    "We use three metrics to evaluate the performance of the model:\n",
    "* R2 Score: It is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. The R2 score ranges from 0 to 1, where 1 indicates a perfect fit and 0 indicates no relationship between the independent and dependent variables.\n",
    "* RMSE (Root Mean Squared Error): It is a measure of the differences between values predicted by a model and the values observed. It is the square root of the average of the squared differences between the predicted and actual values.\n",
    "* MAE (Mean Absolute Error): It is a measure of errors between paired observations expressing the same phenomenon. It is the average of the absolute differences between the predicted and actual values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Results:\n",
      "Average R-squared: 0.9400 (+/- 0.0128)\n",
      "Average RMSE: 12.0909 (+/- 1.5077)\n",
      "Average MAE: 7.6232 (+/- 0.6026)\n",
      "\n",
      "XGB Results:\n",
      "Average R-squared: 0.9913 (+/- 0.0028)\n",
      "Average RMSE: 4.5735 (+/- 0.8450)\n",
      "Average MAE: 1.9955 (+/- 0.2051)\n",
      "\n",
      "RFR Results:\n",
      "Average R-squared: 0.9866 (+/- 0.0025)\n",
      "Average RMSE: 5.7348 (+/- 0.8360)\n",
      "Average MAE: 3.0195 (+/- 0.2764)\n",
      "\n",
      "GBR Results:\n",
      "Average R-squared: 0.9802 (+/- 0.0033)\n",
      "Average RMSE: 6.9842 (+/- 1.0328)\n",
      "Average MAE: 4.3894 (+/- 0.2148)\n",
      "\n",
      "SVR Results:\n",
      "Average R-squared: 0.0619 (+/- 0.0438)\n",
      "Average RMSE: 48.0425 (+/- 3.0118)\n",
      "Average MAE: 31.6738 (+/- 2.4367)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, metrics in results.items():\n",
    "    print(f\"{name} Results:\")\n",
    "    print(f\"Average R-squared: {np.mean(metrics['r2']):.4f} (+/- {np.std(metrics['r2']):.4f})\")\n",
    "    print(f\"Average RMSE: {np.mean(metrics['rmse']):.4f} (+/- {np.std(metrics['rmse']):.4f})\")\n",
    "    print(f\"Average MAE: {np.mean(metrics['mae']):.4f} (+/- {np.std(metrics['mae']):.4f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results of the evaluation metrics, we can determine how well the model is performing and make adjustments as needed to improve its performance. From the five models, LinearRegression performed the best, with the highest R2 score and lowest RMSE and MAE values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Feature Importance\n",
    "\n",
    "We want to check, from all of the features, which features are the most important in predicting the target variable. This can help us understand which features have the most impact on the target variable and how they contribute to the model's predictions.\n",
    "\n",
    "If the coefficients of the features are positive, it means that the feature has a positive impact on the target variable. If the coefficients are negative, it means that the feature has a negative impact (inverse) on the target variable.\n",
    "\n",
    "We will sort the features based on their coefficients to identify the most important features in the model.\n",
    "\n",
    "For the categorical features (with prefix `cat`), there will be a number of categories in the end of the feature. E.g.: `cat_hvac_type_0.0`, means that the feature is `hvac_type` with category `0.0`, in this case is `Fan Coil Units and Central Plant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance(model, model_name, X, y):\n",
    "    feature_names = model.named_steps['preprocessor'].get_feature_names_out()\n",
    "    \n",
    "    if model_name == 'LR':\n",
    "        importances = model.named_steps['regressor'].coef_\n",
    "    elif model_name in ['XGB', 'RFR', 'GBR']:\n",
    "        importances = model.named_steps['regressor'].feature_importances_\n",
    "    elif model_name == 'SVR':\n",
    "        # For SVR, we use permutation importance\n",
    "        perm_importance = permutation_importance(model, X, y, n_repeats=10, random_state=2024)\n",
    "        importances = perm_importance.importances_mean\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    feature_importance = dict(zip(feature_names, importances))\n",
    "    return dict(sorted(feature_importance.items(), key=lambda item: abs(item[1]), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importance for LR:\n",
      "cat__clim_zone_7: 4528797652.4397\n",
      "cat__clim_zone_0A: 4528797629.5039\n",
      "cat__clim_zone_1A: 4528797625.9465\n",
      "cat__clim_zone_6A: 4528797591.2245\n",
      "cat__clim_zone_5A: 4528797585.3367\n",
      "cat__clim_zone_2A: 4528797575.6369\n",
      "cat__clim_zone_3A: 4528797563.1239\n",
      "cat__clim_zone_4A: 4528797545.9550\n",
      "remainder__infilt_ach: 13.1415\n",
      "cat__hvac_type_Fan Coil Units and Central Plant: 6.2366\n",
      "cat__hvac_type_VRF Fan Coils: -6.2366\n",
      "remainder__floor_uval: 4.2216\n",
      "remainder__pv_pe: -3.6500\n",
      "cat__floor_finish_Tiles: -3.1313\n",
      "cat__floor_finish_Hardwood: 2.7539\n",
      "remainder__roof_uval: 2.4075\n",
      "remainder__fac_glaz_uval: 2.1727\n",
      "remainder__lpd: 2.1576\n",
      "remainder__epd: 2.0070\n",
      "remainder__wall_uval: 0.7150\n",
      "cat__wall_type_Brick Plaster: 0.6418\n",
      "cat__wall_type_Precast Concrete: -0.6290\n",
      "cat__roof_type_Concrete: -0.5872\n",
      "cat__roof_type_Metal Deck: 0.5872\n",
      "cat__floor_finish_Carpet: 0.3773\n",
      "remainder__res_hedc: 0.1062\n",
      "remainder__fac_glaz_shgc: -0.0531\n",
      "remainder__pv_pt: 0.0420\n",
      "remainder__pv_po: -0.0341\n",
      "cat__wall_type_Concrete Block: -0.0129\n",
      "remainder__pv_pa: -0.0101\n",
      "remainder__tot_floor_area: -0.0022\n",
      "remainder__build_orient: 0.0017\n",
      "remainder__res_cedc: -0.0006\n",
      "\n",
      "\n",
      "Feature Importance for XGB:\n",
      "cat__clim_zone_7: 0.6753\n",
      "remainder__pv_pa: 0.1561\n",
      "cat__clim_zone_6A: 0.0836\n",
      "cat__hvac_type_Fan Coil Units and Central Plant: 0.0189\n",
      "cat__clim_zone_2A: 0.0165\n",
      "cat__clim_zone_5A: 0.0132\n",
      "cat__roof_type_Concrete: 0.0047\n",
      "cat__clim_zone_4A: 0.0047\n",
      "remainder__res_hedc: 0.0043\n",
      "remainder__pv_pe: 0.0033\n",
      "cat__floor_finish_Tiles: 0.0033\n",
      "remainder__res_cedc: 0.0030\n",
      "remainder__epd: 0.0029\n",
      "cat__clim_zone_3A: 0.0024\n",
      "remainder__lpd: 0.0016\n",
      "remainder__pv_po: 0.0012\n",
      "remainder__infilt_ach: 0.0012\n",
      "cat__clim_zone_0A: 0.0011\n",
      "remainder__fac_glaz_uval: 0.0010\n",
      "remainder__pv_pt: 0.0003\n",
      "remainder__tot_floor_area: 0.0003\n",
      "cat__wall_type_Precast Concrete: 0.0002\n",
      "remainder__fac_glaz_shgc: 0.0002\n",
      "cat__wall_type_Concrete Block: 0.0002\n",
      "cat__wall_type_Brick Plaster: 0.0002\n",
      "cat__clim_zone_1A: 0.0001\n",
      "cat__floor_finish_Hardwood: 0.0001\n",
      "remainder__build_orient: 0.0000\n",
      "cat__floor_finish_Carpet: 0.0000\n",
      "cat__roof_type_Metal Deck: 0.0000\n",
      "cat__hvac_type_VRF Fan Coils: 0.0000\n",
      "remainder__wall_uval: 0.0000\n",
      "remainder__floor_uval: 0.0000\n",
      "remainder__roof_uval: 0.0000\n",
      "\n",
      "\n",
      "Feature Importance for RFR:\n",
      "cat__clim_zone_7: 0.5790\n",
      "remainder__pv_pa: 0.1740\n",
      "cat__clim_zone_6A: 0.0648\n",
      "remainder__res_hedc: 0.0601\n",
      "remainder__res_cedc: 0.0503\n",
      "cat__clim_zone_2A: 0.0220\n",
      "cat__clim_zone_5A: 0.0215\n",
      "cat__hvac_type_VRF Fan Coils: 0.0058\n",
      "cat__hvac_type_Fan Coil Units and Central Plant: 0.0044\n",
      "remainder__pv_pe: 0.0037\n",
      "cat__clim_zone_4A: 0.0022\n",
      "remainder__pv_po: 0.0019\n",
      "cat__clim_zone_3A: 0.0016\n",
      "remainder__epd: 0.0014\n",
      "remainder__fac_glaz_uval: 0.0008\n",
      "remainder__tot_floor_area: 0.0008\n",
      "cat__roof_type_Metal Deck: 0.0008\n",
      "remainder__fac_glaz_shgc: 0.0007\n",
      "remainder__lpd: 0.0006\n",
      "remainder__infilt_ach: 0.0006\n",
      "cat__roof_type_Concrete: 0.0006\n",
      "remainder__roof_uval: 0.0005\n",
      "cat__floor_finish_Tiles: 0.0004\n",
      "remainder__floor_uval: 0.0004\n",
      "remainder__pv_pt: 0.0003\n",
      "cat__clim_zone_0A: 0.0003\n",
      "remainder__build_orient: 0.0001\n",
      "remainder__wall_uval: 0.0001\n",
      "cat__clim_zone_1A: 0.0001\n",
      "cat__floor_finish_Hardwood: 0.0001\n",
      "cat__wall_type_Precast Concrete: 0.0001\n",
      "cat__floor_finish_Carpet: 0.0001\n",
      "cat__wall_type_Concrete Block: 0.0000\n",
      "cat__wall_type_Brick Plaster: 0.0000\n",
      "\n",
      "\n",
      "Feature Importance for GBR:\n",
      "cat__clim_zone_7: 0.5191\n",
      "remainder__pv_pa: 0.1755\n",
      "remainder__res_hedc: 0.1448\n",
      "cat__clim_zone_6A: 0.0418\n",
      "remainder__res_cedc: 0.0329\n",
      "cat__clim_zone_4A: 0.0271\n",
      "cat__clim_zone_2A: 0.0203\n",
      "cat__clim_zone_5A: 0.0154\n",
      "remainder__pv_pe: 0.0050\n",
      "cat__hvac_type_Fan Coil Units and Central Plant: 0.0035\n",
      "remainder__epd: 0.0021\n",
      "cat__roof_type_Concrete: 0.0018\n",
      "remainder__pv_po: 0.0016\n",
      "cat__clim_zone_3A: 0.0016\n",
      "cat__hvac_type_VRF Fan Coils: 0.0014\n",
      "remainder__floor_uval: 0.0010\n",
      "remainder__fac_glaz_shgc: 0.0007\n",
      "remainder__lpd: 0.0007\n",
      "cat__floor_finish_Tiles: 0.0007\n",
      "remainder__roof_uval: 0.0006\n",
      "remainder__fac_glaz_uval: 0.0006\n",
      "remainder__pv_pt: 0.0005\n",
      "remainder__infilt_ach: 0.0005\n",
      "cat__roof_type_Metal Deck: 0.0004\n",
      "cat__clim_zone_1A: 0.0003\n",
      "cat__clim_zone_0A: 0.0002\n",
      "remainder__tot_floor_area: 0.0000\n",
      "cat__wall_type_Brick Plaster: 0.0000\n",
      "cat__wall_type_Concrete Block: 0.0000\n",
      "cat__wall_type_Precast Concrete: 0.0000\n",
      "cat__floor_finish_Carpet: 0.0000\n",
      "cat__floor_finish_Hardwood: 0.0000\n",
      "remainder__build_orient: 0.0000\n",
      "remainder__wall_uval: 0.0000\n",
      "\n",
      "\n",
      "Feature Importance for SVR:\n",
      "remainder__build_orient: 0.1195\n",
      "remainder__tot_floor_area: 0.0094\n",
      "cat__clim_zone_2A: 0.0085\n",
      "remainder__fac_glaz_uval: 0.0004\n",
      "cat__hvac_type_Fan Coil Units and Central Plant: 0.0000\n",
      "cat__clim_zone_1A: 0.0000\n",
      "cat__hvac_type_VRF Fan Coils: 0.0000\n",
      "cat__clim_zone_0A: 0.0000\n",
      "cat__roof_type_Metal Deck: 0.0000\n",
      "cat__floor_finish_Hardwood: 0.0000\n",
      "cat__wall_type_Precast Concrete: 0.0000\n",
      "cat__clim_zone_3A: 0.0000\n",
      "cat__wall_type_Brick Plaster: 0.0000\n",
      "cat__floor_finish_Tiles: 0.0000\n",
      "cat__clim_zone_7: 0.0000\n",
      "cat__wall_type_Concrete Block: 0.0000\n",
      "cat__roof_type_Concrete: 0.0000\n",
      "cat__clim_zone_5A: 0.0000\n",
      "cat__clim_zone_4A: 0.0000\n",
      "cat__clim_zone_6A: 0.0000\n",
      "cat__floor_finish_Carpet: 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nFeature Importance for {name}:\")\n",
    "    importance = get_feature_importance(model, name, X, y)\n",
    "    if importance:\n",
    "        for feature, value in importance.items():\n",
    "            print(f\"{feature}: {value:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Importance Value as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance comparison has been saved to 'feature_importance_comparison.csv'\n"
     ]
    }
   ],
   "source": [
    "def get_feature_importance(model, model_name, X, y):\n",
    "    feature_names = model.named_steps['preprocessor'].get_feature_names_out()\n",
    "    \n",
    "    if model_name == 'LR':\n",
    "        importances = model.named_steps['regressor'].coef_\n",
    "    elif model_name in ['XGB', 'RFR', 'GBR']:\n",
    "        importances = model.named_steps['regressor'].feature_importances_\n",
    "    elif model_name == 'SVR':\n",
    "        # For SVR, we use permutation importance\n",
    "        perm_importance = permutation_importance(model, X, y, n_repeats=10, random_state=2024)\n",
    "        importances = perm_importance.importances_mean\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    return dict(zip(feature_names, importances))\n",
    "\n",
    "# Calculate feature importances for all models\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "feature_importances = {}\n",
    "for name, model in models.items():\n",
    "    feature_importances[name] = get_feature_importance(model, name, X, y)\n",
    "\n",
    "# Create a DataFrame from the feature importances\n",
    "df_importance = pd.DataFrame(feature_importances)\n",
    "\n",
    "# Sort the DataFrame by the average importance across all models\n",
    "df_importance['avg_importance'] = df_importance.mean(axis=1)\n",
    "df_importance = df_importance.sort_values('avg_importance', ascending=False)\n",
    "df_importance = df_importance.drop('avg_importance', axis=1)\n",
    "\n",
    "# Rename the index to include the categorical labels\n",
    "new_index = []\n",
    "for feature in df_importance.index:\n",
    "    if feature.startswith('cat__'):\n",
    "        parts = feature.split('__')\n",
    "        if len(parts) == 3:\n",
    "            new_index.append(f\"{parts[1]}_{parts[2]}\")\n",
    "        else:\n",
    "            new_index.append(feature)\n",
    "    else:\n",
    "        new_index.append(feature)\n",
    "df_importance.index = new_index\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_importance.to_csv('feature_importance_comparison.csv')\n",
    "\n",
    "print(\"Feature importance comparison has been saved to 'feature_importance_comparison.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
